{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot_Stage_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fISBb9Zd3ykR",
        "outputId": "ba2c018b-d212-47ab-ca32-a9e75db7faf9"
      },
      "source": [
        "!pip install text2emotion"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: text2emotion in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: emoji>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from text2emotion) (1.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text2emotion) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->text2emotion) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1v0igrAMmFN"
      },
      "source": [
        "#Imports \n",
        "import numpy as np # Imports numpy for handling numeric data in arrays\n",
        "import pickle # Imports pickle for saving and loading small data\n",
        "import os # Imports  os for file handling\n",
        "import os.path # Imports os.path for directory manipulation\n",
        "import h5py # Imports pickle for saving and loading large data\n",
        "import codecs # Used to handle the dataset\n",
        "import io # Used to read the dataset data\n",
        "import re # Used to clean the text via regular expresssions\n",
        "import zipfile # Package to handle zipped files, specific for the downloaded dataset\n",
        "import requests # Package to request from websites, used to download the dataset\n",
        "import json # Import json package for handling general dataset\n",
        "import yaml # Import yaml package for handling general dataset\n",
        "from tensorflow.keras import Input, Model, models # Import for keras input into models, the model itself and models for loading data \n",
        "from tensorflow.keras.activations import softmax # Import for softmax activaiton function\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense  # Import for embedding, LSTM and dense layers\n",
        "from tensorflow.keras.optimizers import RMSprop # Import for RMSProp automatic optimizeer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # Import to pad sequences to reshape data\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # Import for tokeniing inputs for use\n",
        "from tensorflow.keras.utils import to_categorical, plot_model # Import for one hot encoding data\n",
        "import pip # Imports pip for install\n",
        "import subprocess # Imports subprcoess for calling CMD of google colab\n",
        "import text2emotion as te # Imports text emotion for use"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtWmI5x3KA4m"
      },
      "source": [
        "class ChatbotBase(): # Class to hold information to both classes, acts as an abstract\n",
        "    \n",
        "    #Fields\n",
        "    filterRegex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n\\'0123456789' # Sets up regex characters to filter\n",
        "    hiddenLayers = 232 # Defines unit amount for LSTM layers \n",
        "    batchSize = 64 # Defines the batch size, for the model \n",
        "    epochAmount = 300 # Defines the amount of epochs to use\n",
        "    dataSize = 3 # Int to manage datasize of general data, note that amount should be low (2-3)\n",
        "\n",
        "    trainingModelPath = \"trainingModel/model\" # Defines the path to save and load the model\n",
        "    inferenceModelsPath = \"inferenceModels/\"\n",
        "    \n",
        "    dataFolderDirectory = \"data/\" # Defines data folder\n",
        "\n",
        "    sDatasetUrl = 'https://github.com/Lewis-C/EHU-CIS3140-Project-Artefact/raw/main/Datasets/Knowledge%20Base%20(Modified).zip' # Defines specific data url\n",
        "    sDatasetFilesPath = \"Knowledge Base (Modified)/\" # Defines specific data path\n",
        "    sDatasetDirectory = f\"{dataFolderDirectory}{sDatasetFilesPath}/\" # Defines specific data direcotry\n",
        "\n",
        "    gDatasetUrl = 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json' # Defines general data url\n",
        "    gDatasetFile =  f\"{dataFolderDirectory}train-v2.0.json\" # Defines general data file\n",
        "    \n",
        "    beginningOfSentence = \"bos \" # Defines the beginning of text marker (Bos used instead of start for example to avoid accidental classifications)\n",
        "    endOfSentence = \" eos\" # Defines the end of text marker\n",
        "    \n",
        "    def PickleSave(item, fileName): # Function to save items with picckle\n",
        "        with open(f'{ChatbotBase.dataFolderDirectory}{fileName}.pickle', 'wb') as f: # Opens directory\n",
        "            pickle.dump(item, f, protocol=pickle.HIGHEST_PROTOCOL) # Saves item \n",
        "            \n",
        "    def PickleOpen(item, fileName): # Function to open items with picckle\n",
        "        with open(f'{ChatbotBase.dataFolderDirectory}{fileName}.pickle', 'rb') as f: # Opens directory\n",
        "            item = pickle.load(f) # loads item\n",
        "            return item # Returns item\n",
        "        \n",
        "    def HDF5Save(item, fileName): # Function to save items with HDF5\n",
        "        with h5py.File(f'{ChatbotBase.dataFolderDirectory}{fileName}.hdf5', 'w') as f: # Opens directory\n",
        "            f.create_dataset(fileName, data=item) # Saves item\n",
        "            \n",
        "    def HDF5Open(item, fileName): # Function to open items with HDF5\n",
        "        with h5py.File(f'{ChatbotBase.dataFolderDirectory}{fileName}.hdf5', 'r') as f: # Opens directory\n",
        "            item = f.get(fileName)[()] # loads item as numpy array\n",
        "            return item # returns item\n",
        "        \n",
        "    def CleanText(text): # Function for cleaning text, removing unneccesary characters and making lower case\n",
        "        \n",
        "        if type(text) == dict: # Makes sure text is not a dictionary (Common error due to : use in text)\n",
        "            keys = list(text) # Gets the key (note there should only be one )\n",
        "            totalText = \"\" # Empty to hold the entire text\n",
        "            for key in keys: # For each key\n",
        "                value = text[key] # Finds the value associaaed \n",
        "                text = f\"{key} + {value}\" # Appends data back together without colon\n",
        "                totalText = totalText + text # adds to total string\n",
        "            text = totalText # makes text the total string\n",
        "            \n",
        "        text = text.lower() # Converts text input to lowercase\n",
        "        \n",
        "        text = re.sub(r\"i'm\", \"i am\", text) # Removes potentially conflicting punctuation, swapping with full text\n",
        "        text = re.sub(r\"he's\", \"he is\", text)\n",
        "        text = re.sub(r\"she's\", \"she is\", text)\n",
        "        text = re.sub(r\"it's\", \"it is\", text)\n",
        "        text = re.sub(r\"that's\", \"that is\", text)\n",
        "        text = re.sub(r\"what's\", \"that is\", text)\n",
        "        text = re.sub(r\"where's\", \"where is\", text)\n",
        "        text = re.sub(r\"how's\", \"how is\", text)\n",
        "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "        text = re.sub(r\"\\'re\", \" are\", text)\n",
        "        text = re.sub(r\"\\'d\", \" would\", text)\n",
        "        text = re.sub(r\"\\'re\", \" are\", text)\n",
        "        text = re.sub(r\"won't\", \"will not\", text)\n",
        "        text = re.sub(r\"can't\", \"cannot\", text)\n",
        "        text = re.sub(r\"n't\", \" not\", text)\n",
        "        text = re.sub(r\"n'\", \"ng\", text)\n",
        "        text = re.sub(r\"'bout\", \"about\", text)\n",
        "        text = re.sub(r\"'til\", \"until\", text)\n",
        "        text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text) # Removes conflicting characters completely\n",
        "        \n",
        "        return text # Return cleaned text\n",
        "    \n",
        "    def ProcessSpecificDataset(): # Function for processing the specific datset\n",
        "        queries = [] # Declares empty list for queries\n",
        "        responses = [] # Declares empty list for responses\n",
        "        taggedResponses = [] # Declares empty list for tagged responses\n",
        "        sDatasetFilesList = os.listdir(ChatbotBase.sDatasetDirectory)\n",
        "        for file in sDatasetFilesList: # For file in the list of files for dataset (there is one for each category)\n",
        "            with open(ChatbotBase.sDatasetDirectory + file, \"rb\") as f: # Open the file\n",
        "                data = yaml.safe_load(f) # Load format specific into data\n",
        "                conversations = data['conversations'] # Take each \"conversation\" into dialogues list\n",
        "            for conversation in conversations: # For every conversation in conversations\n",
        "                if len(conversation) > 2: # If the conversation has more than one line (meaning multiple potential responses)\n",
        "                    queries.append(ChatbotBase.CleanText(conversation[0])) # Queries is appended the one query\n",
        "                    queryResponses = '' # String for holding responses\n",
        "                    for response in conversation[1:]: # For every response\n",
        "                        queryResponses += \" \" + (ChatbotBase.CleanText(response)) # Cleans text then concatenates\n",
        "                    responses.append(queryResponses) # Adds this to the responses list\n",
        "                elif len(conversation) == 2: # If the conversation only has two lines \n",
        "                    queries.append(ChatbotBase.CleanText(conversation[0])) # Adds the initial line as a query\n",
        "                    responses.append(ChatbotBase.CleanText(conversation[1])) # Adds the response as a response\n",
        "                \n",
        "\n",
        "        for i, response  in enumerate(responses): # For every response (gets index as well)\n",
        "            if (type(response) == str): # If the response is a string (verificaiton, may not be neccesary as clean text has dictionary handling)\n",
        "                taggedResponses.append(f\"{ChatbotBase.beginningOfSentence}{response}{ChatbotBase.endOfSentence}\") # Adds to the tagged responses with tags added\n",
        "            else: # If not a string\n",
        "                queries.pop(i) # Pops\n",
        "\n",
        "        encoderInputs = queries # Defines inputs for encoder using the queries cleaned\n",
        "        decoderInputs = taggedResponses # Defines inputs for decoder the \n",
        "        \n",
        "        return encoderInputs, decoderInputs # Return the encoder inputs and decoder inputs\n",
        "\n",
        "    def ProcessGeneralDataset(): # Function for processing general data\n",
        "        queries = [] # Defines empty list for queries\n",
        "        responses = [] # Defines empty list for responses\n",
        "        taggedResponses = [] # Defines empty list for tagged response\n",
        "        with open(ChatbotBase.gDatasetFile, \"rb\") as f: # With the dataset file ope\n",
        "            data = json.load(f) # Loaded file data\n",
        "            for topic in data[\"data\"]: # For each topic in the data\n",
        "                for paragraph in topic[\"paragraphs\"][:ChatbotBase.dataSize]: # For each paragraph (note that each topic only gets three to give variety)\n",
        "                    for qAndA in paragraph['qas']: # For question and answer\n",
        "                        try: # Try to\n",
        "                            responses.append(ChatbotBase.CleanText(qAndA[\"answers\"][0][\"text\"])) # Get answer first (some questions have no answers)\n",
        "                            queries.append(ChatbotBase.CleanText(qAndA['question'])) # Then get the questions, cleaning at the same time\n",
        "                        except:\n",
        "                            continue # If failure, skip the qanda\n",
        "        for response in responses: # For every response\n",
        "            taggedResponses.append(f\"{ChatbotBase.beginningOfSentence}{response}{ChatbotBase.endOfSentence}\") # Adds to the tagged responses with tags added\n",
        "        \n",
        "        encoderInputs = queries # Defines inputs for encoder using the queries cleaned\n",
        "        decoderInputs = taggedResponses # Defines inputs for decoder the \n",
        "        \n",
        "        return encoderInputs, decoderInputs # Return the encoder inputs and decoder inputs\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvaIS4km7zjm"
      },
      "source": [
        "class ChatbotTrain(ChatbotBase): # Class for training chatbot \n",
        "    wordIndex = {} # Defines empty values to load into \n",
        "    indexWord = {}\n",
        "    encoderInputData = []\n",
        "    decoderInputData = []\n",
        "    decoderOutputData = []\n",
        "\n",
        "    def __init__(self): # Defines the class constructor\n",
        "        try: # Tries to load data from files to avoid prcessing time\n",
        "            print(\"Attempting to find existing files\")\n",
        "            wordIndex = ChatbotBase.PickleOpen(self.wordIndex, \"wordIndex\") # Uses pickle load for word index dictionary\n",
        "            indexWord = ChatbotBase.PickleOpen(self.indexWord, \"indexWord\") # Uses pickle load for index word dictionary\n",
        "            encoderInputData = ChatbotBase.HDF5Open(self.encoderInputData, \"encoderInputData\")  # Uses HDF5 load for encoder input data numpy array\n",
        "            decoderInputData = ChatbotBase.HDF5Open(self.decoderInputData, \"decoderInputData\") # Uses HDF5 load for decoder input data numpy array\n",
        "            decoderOutputData = ChatbotBase.HDF5Open(self.decoderOutputData, \"decoderOutputData\") # Uses HDF5 load for decoder output data numpy array\n",
        "            print(\"Existing Data Found\")\n",
        "        except: # If file load files\n",
        "            if ((os.path.isfile(ChatbotBase.gDatasetFile)) and (os.path.isfile(ChatbotBase.sDatasetDirectory + \"ai.yml\"))): # If datasets files are already donwloaded\n",
        "                print(\"Dataset found, Processing\") \n",
        "            else: # Otherwise download file and store accordingly \n",
        "                print(\"File not found, downloading. This may take some time.\")\n",
        "                datasetRequest = requests.get(ChatbotBase.sDatasetUrl) # Uses url to get a HTTP request for the data and hold it as the defined r, deownloading the content\n",
        "                datasetContent = zipfile.ZipFile(io.BytesIO(datasetRequest.content)) # Defines the downloaded content as zip file\n",
        "                datasetContent.extractall(path=ChatbotBase.dataFolderDirectory) # Extracts the file into the script directry\n",
        "                datasetRequest = requests.get(ChatbotBase.gDatasetUrl) # Uses url to get a HTTP request for the data and hold it as the defined r, deownloading the content\n",
        "                datasetContent = datasetRequest.json()\n",
        "                with open(ChatbotBase.gDatasetFile, 'w') as f:\n",
        "                    json.dump(datasetContent, f)\n",
        "                print(\"Dataset Downloaded, Processing\")\n",
        "            processedData = self.ProcessDataset() # Calls dataset process function, stores output list \n",
        "            wordIndex = processedData[0] # Gets each item from the list and stores it to specific variable \n",
        "            ChatbotBase.PickleSave(wordIndex, \"wordIndex\") # Saves for future use\n",
        "            indexWord = processedData[1]\n",
        "            ChatbotBase.PickleSave(indexWord, \"indexWord\")\n",
        "            encoderInputData = processedData[2]\n",
        "            ChatbotBase.HDF5Save(encoderInputData, \"encoderInputData\")\n",
        "            decoderInputData = processedData[3]\n",
        "            ChatbotBase.HDF5Save(decoderInputData, \"decoderInputData\")\n",
        "            decoderOutputData = processedData[4]\n",
        "            ChatbotBase.HDF5Save(decoderOutputData, \"decoderOutputData\")\n",
        "        finally: # Regardless of route\n",
        "            vocabSize = len(wordIndex) + 1 # Find the vocab size from the word index \n",
        "            encoderMaxLength = len(encoderInputData[0]) # Find the encoders max length\n",
        "            decoderMaxLength = len(decoderInputData[0]) # Find the decoders max length\n",
        "            ChatbotBase.PickleSave(encoderMaxLength, \"encoderMaxLength\") # Saves for use in web app\n",
        "            ChatbotBase.PickleSave(decoderMaxLength, \"decoderMaxLength\")\n",
        "            print(\"Complete\")\n",
        "    \n",
        "        trainingModel = self.MakeChatbotModel(vocabSize) # Makes training model\n",
        "        \n",
        "        learningHistory = trainingModel.fit(x = [encoderInputData, decoderInputData], y = decoderOutputData, batch_size = ChatbotBase.batchSize, epochs = ChatbotBase.epochAmount, validation_split = 0.2) # Fits model to data with parameters selected and reccomended validation split of 0.2\n",
        "        learningHistoryData = learningHistory.history # Stores the history of the process\n",
        "        trainingModel.save(ChatbotBase.trainingModelPath) # Saves the model\n",
        "        ChatbotBase.PickleSave(learningHistoryData, \"history\") # Saves the history\n",
        "\n",
        "    def ProcessDataset(self):\n",
        "    \n",
        "        specificData = ChatbotBase.ProcessSpecificDataset() # Cals the processing of specific dataset\n",
        "        encoderInputs = specificData[0] # Gets the encoder inputs\n",
        "        decoderInputs = specificData[1] # Gets the decoder inputs\n",
        "            \n",
        "        generalData = ChatbotBase.ProcessGeneralDataset() # Calls the processing of general dataset\n",
        "        encoderInputs = encoderInputs + (generalData[0]) # Adds the outputs to the encoder inputs\n",
        "        decoderInputs = decoderInputs + (generalData[1]) # Adds the outputs to the decoder inputs\n",
        "\n",
        "        print(\"Emotionally Analysing training data\")\n",
        "        encoderInputEmotionData = np.zeros(((len(encoderInputs),5))) # Creates empty list for holding emotional input data of each encoder inpout\n",
        "        for encoderIndex, encoderInput in enumerate(encoderInputs): # For loop enurmerating each encoder input\n",
        "            currentEmotionList = np.zeros((5)) # Creates empty array for each encoder inpout\n",
        "            emotionDictionary = te.get_emotion(encoderInput) # Establishes an emotion dictionary of each input\n",
        "            for emotionIndex, emotion in enumerate(emotionDictionary): # For each emotion in said dictiionary\n",
        "                currentEmotionList[emotionIndex] = (emotionDictionary[emotion]) # Add to the current input list\n",
        "            encoderInputEmotionData[encoderIndex] = currentEmotionList # Add current input list to total input list\n",
        "        print(\"Emotional Analysis Complete\")\n",
        "\n",
        "        fullText = np.concatenate((encoderInputs,decoderInputs)) # Joins the two lists as full text \n",
        "\n",
        "        tokenizer = Tokenizer(filters=ChatbotBase.filterRegex) # Defines tokenizer with filter of incompatible regex character\n",
        "        tokenizer.fit_on_texts(fullText) # Fits filter on joined text\n",
        "            \n",
        "        wordIndex = tokenizer.word_index # Defines a word index using tokenizer\n",
        "            \n",
        "        vocabSize = len(wordIndex) + 1 # Finds the vocab size\n",
        "\n",
        "        indexWord = {} # Defines index for index word, structured with index as key instead of word\n",
        "        for word, index in wordIndex.items(): # For every word and index in word index\n",
        "            indexWord[index] = word # Add to reversed index\n",
        "            \n",
        "        encoderSequences = tokenizer.texts_to_sequences(encoderInputs) # Converts encoder inputs to sequences using tokenizer\n",
        "        for i in range(len(encoderSequences)): # For every encoder input\n",
        "            encoderEmotionSequence = np.append(encoderSequences[i], encoderInputEmotionData[i]) # Append the relevant emotional data\n",
        "            encoderSequences[i] = encoderEmotionSequence # adds the emotion sequence to the encoder sequence\n",
        "\n",
        "        decoderSequences = tokenizer.texts_to_sequences(decoderInputs) # Converts encoder inputs to sequences using tokenizer\n",
        "            \n",
        "        encoderMaxLength = max([len(sequence) for sequence in encoderSequences]) # Finds the max length of the encoder sequences\n",
        "        encoderInputData = pad_sequences(encoderSequences, maxlen=encoderMaxLength, padding='post') # Pads the data according to the max, making everything the same length \n",
        "            \n",
        "        decoderMaxLength = max([len(sequence) for sequence in decoderSequences]) # Finds the max length of the decoder sequences\n",
        "        decoderInputData = pad_sequences(decoderSequences, maxlen=decoderMaxLength, padding='post')  # Pads the data according to the max, making everything the same length ]\n",
        "            \n",
        "        for i in range(len(decoderSequences)): # For each item in tokenized answers (Not encoder fit)\n",
        "            decoderSequences[i] = decoderSequences[i][1:] # Remove start tag\n",
        "        decoderOutputPadding = pad_sequences(decoderSequences, maxlen=decoderMaxLength, padding='post') # Pads answers\n",
        "        decoderOutputData = to_categorical(decoderOutputPadding, vocabSize, dtype = 'b') # categorises data, making each sequence a list of lists where 0 represents nothing and 1 represents a wird \n",
        "            \n",
        "        return wordIndex, indexWord, encoderInputData, decoderInputData, decoderOutputData\n",
        "\n",
        "    def MakeChatbotModel(self, vocabSize): # Function to construct the chatbot model \n",
        "        # Encoder Layer Definitions\n",
        "        encoderInputLayer = Input(shape=(None,), name = \"Encoder_Input_Layer\") # Defines input layer with flexible shape\n",
        "        encoderEmbeddingLayer = Embedding(vocabSize, ChatbotBase.hiddenLayers, mask_zero=True, name = \"Encoder_Embedding_Layer\") # Defines embedding layer, using the amount of unique words, hidden layers defined and ignoring 0s to create embedding matrix of entire words uses\n",
        "        encoderLSTMLayer = LSTM(ChatbotBase.hiddenLayers, return_state=True, dropout=0.5, name = \"Encoder_LSTM_Layer\") # Defines the LSTM of the encoder with use of hidden layers, and returning values for use in the decoder\n",
        "        #Decoder Layer Definitions        \n",
        "        decoderInputLayer = Input(shape=(None,), name = \"Decoder_Input_Layer\") # Defines input layer with flexible shape\n",
        "        decoderEmbeddingLayer = Embedding(vocabSize, ChatbotBase.hiddenLayers, mask_zero=True, name = \"Decoder_Embedding_Layer\")  # Defines embedding layer, using the amount of unique words, hidden layers defined and ignoring 0s to create embedding matrix of entire words uses\n",
        "        decoderLSTMLayer = LSTM(ChatbotBase.hiddenLayers, return_state=True, return_sequences=True, dropout=0.5, name = \"Decoder_LSTM_Layer\") # Defines the LSTM of the decoder with use of hidden layers, and returning values for use in the dense layer\n",
        "        decoderDenseLayer = Dense(vocabSize, activation=softmax, name = \"Decoder_Dense_Layer\") # Takes the LSTM data to find the numeric weightings for each value\n",
        "        \n",
        "        #Encoder\n",
        "        encoderInputs = encoderInputLayer # Uses input layer definition to handle encoder inputs\n",
        "        encoderEmbedding = encoderEmbeddingLayer(encoderInputs) # Uses embedding layer definition to handle encoder embedding\n",
        "        _, stateH, stateC = encoderLSTMLayer(encoderEmbedding) # Uses lstm layer definition to handle encoder process and returns as states\n",
        "        encoderStates = [stateH, stateC] # List to hold the states of the encoder training for interpreation use and decoder use\n",
        "        \n",
        "        #Decoder\n",
        "        decoderInputs = decoderInputLayer # Uses input layer definition to handle decoder inputs\n",
        "        decoderEmbedding = decoderEmbeddingLayer(decoderInputs) # Uses input layer definition to handle decoder inputs\n",
        "        decoderOutputs, _, _ = decoderLSTMLayer(decoderEmbedding, initial_state=encoderStates) # Uses lstm layer definition to handle deocer process and returns as states\n",
        "        decoderOutput = decoderDenseLayer(decoderOutputs) # Uses dense layer to get numeric weightings\n",
        "        \n",
        "        chatbotModel = Model(inputs = [encoderInputs, decoderInputs], outputs = decoderOutput, name = \"Chatbot_Model\") # Defines the layers as a single model\n",
        "        chatbotModel.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics = ['accuracy']) # Compiles the model with optimizer RMSProp, loss function and accuracy monitored\n",
        "        \n",
        "        #chatbotModel.summary(line_length=220) # Summarises model\n",
        "        if not os.path.exists('figures'): # If the directory doesnt exist\n",
        "            os.makedirs('figures') # Create it\n",
        "        plot_model(chatbotModel, to_file=\"figures/model.png\", show_shapes=True) # Saves the model plot to file \n",
        "        return chatbotModel # Returns for use"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjQXe-tfMx7c"
      },
      "source": [
        "class ChatbotRun(ChatbotBase): # Class for using chatbot \n",
        "    wordIndex = {} # Defines empty values to load into \n",
        "    indexWord = {}\n",
        "    encoderInputData = []\n",
        "    decoderInputData = []\n",
        "\n",
        "    def __init__(self): # Defines the class constructor\n",
        "        if (not os.path.exists(ChatbotBase.trainingModelPath)): # If model file doesnt exist\n",
        "            print(\"Training Model\")\n",
        "            trainingProcess = ChatbotTrain() # Instance the training model class\n",
        "        try: # Tries to load data from files to avoid prcessing time\n",
        "            print(\"Attempting to find existing files\")\n",
        "            wordIndex = ChatbotBase.PickleOpen(self.wordIndex, \"wordIndex\")  # Tries to load data from files to avoid prcessing time\n",
        "            indexWord = ChatbotBase.PickleOpen(self.indexWord, \"indexWord\") # Tries to load data from files to avoid prcessing time\n",
        "            encoderInputData = ChatbotBase.HDF5Open(self.encoderInputData, \"encoderInputData\")  # Uses HDF5 load for encoder input data numpy array\n",
        "            decoderInputData = ChatbotBase.HDF5Open(self.decoderInputData, \"decoderInputData\") # Uses HDF5 load for decoder input data numpy array\n",
        "            print(\"Existing Data Found\")\n",
        "        except:\n",
        "            if ((os.path.isfile(ChatbotBase.gDatasetFile)) and (os.path.isfile(ChatbotBase.sDatasetDirectory + \"ai.yml\"))): # If datasets files are already donwloaded\n",
        "                print(\"Dataset found, Processing\") \n",
        "            else: # Otherwise download file and store accordingly \n",
        "                print(\"File not found, downloading. This may take some time.\")\n",
        "                datasetRequest = requests.get(ChatbotBase.sDatasetUrl) # Uses url to get a HTTP request for the data and hold it as the defined r, deownloading the content\n",
        "                datasetContent = zipfile.ZipFile(io.BytesIO(datasetRequest.content)) # Defines the downloaded content as zip file\n",
        "                datasetContent.extractall(path=ChatbotBase.dataFolderDirectory) # Extracts the file into the script directry\n",
        "                datasetRequest = requests.get(ChatbotBase.gDatasetUrl) # Uses url to get a HTTP request for the data and hold it as the defined r, deownloading the content\n",
        "                datasetContent = datasetRequest.json()\n",
        "                with open(ChatbotBase.gDatasetFile, 'w') as f:\n",
        "                    json.dump(datasetContent, f)\n",
        "                print(\"Dataset Downloaded, Processing\")\n",
        "            wordIndex, indexWord, encoderInputData, decoderInputData  = self.ProcessDataset() # Calls dataset process function, stores output into values\n",
        "            ChatbotBase.PickleSave(wordIndex, \"wordIndex\") # Saves for future use\n",
        "            ChatbotBase.PickleSave(indexWord, \"indexWord\")\n",
        "            ChatbotBase.HDF5Save(encoderInputData, \"encoderInputData\")\n",
        "            ChatbotBase.HDF5Save(decoderInputData, \"decoderInputData\")\n",
        "        finally:\n",
        "            vocabSize = len(wordIndex) + 1 # Find the vocab size from the word index \n",
        "            encoderMaxLength = len(encoderInputData[0])  # Find the encoders max length\n",
        "            decoderMaxLength = len(decoderInputData[0])  # Find the decoders max length\n",
        "            ChatbotBase.PickleSave(encoderMaxLength, \"encoderMaxLength\") # Saves for use in web app\n",
        "            ChatbotBase.PickleSave(decoderMaxLength, \"decoderMaxLength\")\n",
        "\n",
        "            print(\"Complete\")\n",
        "\n",
        "        self.MakeInferenceModels() # Makes inference models\n",
        "\n",
        "    def ProcessDataset(self):\n",
        "    \n",
        "        specificData = ChatbotBase.ProcessSpecificDataset() # Cals the processing of specific dataset\n",
        "        encoderInputs = specificData[0] # Gets the encoder inputs\n",
        "        decoderInputs = specificData[1] # Gets the decoder inputs\n",
        "            \n",
        "        generalData = ChatbotBase.ProcessGeneralDataset() # Calls the processing of general dataset\n",
        "        encoderInputs = encoderInputs + (generalData[0]) # Adds the outputs to the encoder inputs\n",
        "        decoderInputs = decoderInputs + (generalData[1]) # Adds the outputs to the decoder inputs\n",
        "            \n",
        "        fullText = np.concatenate((encoderInputs,decoderInputs)) # Joins the two lists as full text \n",
        "\n",
        "        tokenizer = Tokenizer(filters=ChatbotBase.filterRegex) # Defines tokenizer with filter of incompatible regex character\n",
        "        tokenizer.fit_on_texts(fullText) # Fits filter on joined text\n",
        "            \n",
        "        wordIndex = tokenizer.word_index # Defines a word index using tokenizer\n",
        "            \n",
        "        vocabSize = len(wordIndex) + 1 # Finds the vocab size\n",
        "\n",
        "        indexWord = {} # Defines index for index word, structured with index as key instead of word\n",
        "        for word, index in wordIndex.items(): # For every word and index in word index\n",
        "            indexWord[index] = word # Add to reversed index\n",
        "            \n",
        "        encoderSequences = tokenizer.texts_to_sequences(encoderInputs) # Converts encoder inputs to sequences using tokenizer\n",
        "        decoderSequences = tokenizer.texts_to_sequences(decoderInputs) # Converts encoder inputs to sequences using tokenizer\n",
        "            \n",
        "        encoderMaxLength = max([len(sequence) for sequence in encoderSequences]) # Finds the max length of the encoder sequences\n",
        "        encoderInputData = pad_sequences(encoderSequences, maxlen=encoderMaxLength, padding='post') # Pads the data according to the max, making everything the same length \n",
        "            \n",
        "        decoderMaxLength = max([len(sequence) for sequence in decoderSequences]) # Finds the max length of the decoder sequences\n",
        "        decoderInputData = pad_sequences(decoderSequences, maxlen=decoderMaxLength, padding='post')  # Pads the data according to the max, making everything the same length ]\n",
        "            \n",
        "        return wordIndex, indexWord, encoderInputData, decoderInputData\n",
        "        \n",
        "\n",
        "    def MakeInferenceModels(self):\n",
        "\n",
        "        # Original trained model use\n",
        "        trainedModel = models.load_model(ChatbotBase.trainingModelPath) # Loads trained model\n",
        "        encoderInputs = trainedModel.get_layer(\"Encoder_Input_Layer\").input # Gets the encoder inputs \n",
        "        encoderEmbeddingLayer = trainedModel.get_layer(\"Encoder_Embedding_Layer\") # Gets the encoder embedding layer definition\n",
        "        encoderEmbedding = encoderEmbeddingLayer(encoderInputs) # Finds encoder input embeddings\n",
        "        encoderLSTMLayer = trainedModel.get_layer(\"Encoder_LSTM_Layer\") # Gets the encoder ltm layer\n",
        "        _, stateH, stateC = encoderLSTMLayer(encoderEmbedding) # Gets the encoder states\n",
        "        encoderStates = [stateH, stateC] # Stores as list for use\n",
        "        decoderInputs = trainedModel.get_layer(\"Decoder_Input_Layer\").input # Gets the decoder inputs \n",
        "        decoderLSTMLayer = trainedModel.get_layer(\"Decoder_LSTM_Layer\") # Gets the deocoder ltm layer\n",
        "        decoderEmbeddingLayer = trainedModel.get_layer(\"Decoder_Embedding_Layer\") # Gets the decooder embedding layer definition\n",
        "        decoderEmbedding = decoderEmbeddingLayer(decoderInputs) # # Finds decoder input embeddings\n",
        "        decoderDenseLayer = trainedModel.get_layer(\"Decoder_Dense_Layer\") # Gets the decooder dense layer definition\n",
        "\n",
        "\n",
        "        # Model Definitions\n",
        "        stateHInputLayer = Input(shape=(ChatbotBase.hiddenLayers,), name = \"State_H_Input_Layer\") # Input layer for state h, uses hidden layers\n",
        "        stateCInputLayer = Input(shape=(ChatbotBase.hiddenLayers,), name = \"State_C_Input_Layer\") # Input layer for state c, uses hidden layers\n",
        "        \n",
        "        # Encoder Model\n",
        "        encoderModel = Model(inputs=encoderInputs, outputs=encoderStates, name = \"Inference_Encoder_Model\") # Model for encoder of inference model, takes inputs and produces encodser states, uses trained model encoder inputs\n",
        "        #encoderModel.summary(line_length=220) # Summary of encoder model, commented out for use\n",
        "        \n",
        "        #Decoder Inference Model\n",
        "        decoderStateInputH = stateHInputLayer # Uses state h input layer as decoder input\n",
        "        decoderStateInputC = stateCInputLayer # Uses state c input layer as decoder input\n",
        "        decoderStateInputs = [decoderStateInputH, decoderStateInputC] # Creates list of states\n",
        "        decoderOutputs, stateH, stateC = decoderLSTMLayer(decoderEmbedding, initial_state=decoderStateInputs) #  Uses training model LSTM layer definition with the embeddings to find the outputs of the states input\n",
        "        decoderStates = [stateH, stateC] # Takes those states as a list for use in next iteration\n",
        "        decoderOutputs = decoderDenseLayer(decoderOutputs) # Condenses outputs into weights, giving probablity of text \n",
        "        decoderModel = Model(inputs = [decoderInputs] + decoderStateInputs, outputs = [decoderOutputs] + decoderStates, name = \"Inference_Decoder_Model\") # Createes the decoder inferance model using training input and outputs as well as the states\n",
        "        #decoderModel.summary(line_length=220) # Summarises model, commented out for use\n",
        "\n",
        "        decoderModel.save(f\"{ChatbotBase.inferenceModelsPath}decoderModel.h5\") # Saves the model for use in webapp\n",
        "        encoderModel.save(f\"{ChatbotBase.inferenceModelsPath}encoderModel.h5\") # Saves the model for use in webapp"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e24RAK4Mx2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b9a90d-a81d-458c-bbad-62d754ab708c"
      },
      "source": [
        "chatbotProcess = ChatbotRun() # Otherwise just run the chatbot"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attempting to find existing files\n",
            "Existing Data Found\n",
            "Complete\n",
            "Model: \"Inference_Decoder_Model\"\n",
            "____________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
            "Layer (type)                                                            Output Shape                                     Param #                   Connected to                                                             \n",
            "============================================================================================================================================================================================================================\n",
            "Decoder_Input_Layer (InputLayer)                                        [(None, None)]                                   0                                                                                                  \n",
            "____________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
            "Decoder_Embedding_Layer (Embedding)                                     (None, None, 232)                                2728088                   Decoder_Input_Layer[0][0]                                                \n",
            "____________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
            "State_H_Input_Layer (InputLayer)                                        [(None, 232)]                                    0                                                                                                  \n",
            "____________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
            "State_C_Input_Layer (InputLayer)                                        [(None, 232)]                                    0                                                                                                  \n",
            "____________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
            "Decoder_LSTM_Layer (LSTM)                                               [(None, None, 232), (None, 232), (None, 232)]    431520                    Decoder_Embedding_Layer[1][0]                                            \n",
            "                                                                                                                                                   State_H_Input_Layer[0][0]                                                \n",
            "                                                                                                                                                   State_C_Input_Layer[0][0]                                                \n",
            "____________________________________________________________________________________________________________________________________________________________________________________________________________________________\n",
            "Decoder_Dense_Layer (Dense)                                             (None, None, 11759)                              2739847                   Decoder_LSTM_Layer[1][0]                                                 \n",
            "============================================================================================================================================================================================================================\n",
            "Total params: 5,899,455\n",
            "Trainable params: 5,899,455\n",
            "Non-trainable params: 0\n",
            "____________________________________________________________________________________________________________________________________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}