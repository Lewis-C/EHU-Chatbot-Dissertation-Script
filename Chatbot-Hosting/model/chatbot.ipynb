{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDbrTO-VQ1jl",
        "outputId": "b913b411-80f2-43a3-dd0e-8f66e9ed9f35"
      },
      "source": [
        "#!pip install text2emotion # Install of text2emotion, unneccessary for webapp hosteed"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: text2emotion in /usr/local/lib/python3.7/dist-packages (0.0.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from text2emotion) (3.2.5)\n",
            "Requirement already satisfied: emoji>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from text2emotion) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->text2emotion) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgSQHcH-OmfG",
        "outputId": "232b10d3-b52b-404c-ea33-5e6931112ff7"
      },
      "source": [
        "from tensorflow.keras import models # Import models for laoding and prediction\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # Import to pad sequences to reshape data\n",
        "import numpy as np # Import numpy for list management\n",
        "import text2emotion as te # Import text2emotion for emotion classificaiton of inputs\n",
        "import re # Import regular expressions for cleaning text\n",
        "import pickle # Import pickle for file opening"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTqJB0zIO3ff"
      },
      "source": [
        "def ChatbotInputHandling(userInput):  # Funciton to take user input and process for response   \n",
        "    if userInput.startswith('my name is'): # Rules based implementation to handle name \n",
        "        name = userInput[11:] # Takes user name from their input\n",
        "        chatbotOutput = (f\"Hi {name}, I am a chatbot designed to offer emotional support and informational support about Edge Hill University. Try asking me \\\"where is the catalyst \\\".\") # Responds with introduction\n",
        "    elif userInput.startswith('hi my name is'): # Rules based implementation to handle name \n",
        "        name = userInput[14:] # Takes user name from their input\n",
        "        chatbotOutput = (f\"Hello {name}, I am a chatbot designed to offer emotional support and informational support about Edge Hill University. Try asking me \\\"where is the catalyst \\\".\") # Responds with introduction\n",
        "    else: # If not any rule matched\n",
        "        processedInput = TextToTokens(userInput) # Processes user input\n",
        "        if (processedInput[0][0] == 0): # If the chatbot can't recognise what was said\n",
        "            chatbotOutput = (f\"Sorry I didn't catch what you said\") # Prints default message\n",
        "        else:\n",
        "            statesValues = encoderModel.predict(processedInput) # Takes input and uses encoder model to find the states for the input\n",
        "            targetSequence = np.zeros((1, 1)) # Defines an empty list for holding the previous state\n",
        "            targetSequence[0, 0] = wordIndex['bos'] # Initilisaes with the start marker\n",
        "            stopCondition = False # Sets boolean to handle stopping the iteration\n",
        "            chatbotOutput = '' # Empty string for holding the predicted text\n",
        "            while not stopCondition: # While not stopping\n",
        "                decoderOutputs, h, c = decoderModel.predict([targetSequence] + statesValues) # Use states and previous state to predict word weightings\n",
        "                sampledWordIndex = np.argmax(decoderOutputs[0, -1, :]) # Finds the locaiton of the most predicted word\n",
        "                sampledWord = None # Originally defines a word sample as none\n",
        "                if sampledWordIndex in indexWord: # If the location exsits in the reversed index\n",
        "                    word = indexWord[sampledWordIndex] # Word is made the value for said location\n",
        "                    if word != 'eos': # If that word is not the end marker\n",
        "                        chatbotOutput += f' {word}' # Add the word to the translation\n",
        "                    sampledWord = word # sets word to the sample word for future use\n",
        "            \n",
        "                    if sampledWord == 'eos' or len(chatbotOutput.split()) > decoderMaxLength: # If the end marker is met or the max length is met\n",
        "                        stopCondition = True # Stop predicting\n",
        "            \n",
        "                    targetSequence = np.zeros((1, 1)) # Emptys target liust\n",
        "                    targetSequence[0, 0] = sampledWordIndex # Makes the list the last sampled word's index\n",
        "                    statesValues = [h, c] # Sets the states values to those found in the decoder iteration\n",
        "    \n",
        "    return chatbotOutput # returns the output to the app the output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnw16WUwPKou"
      },
      "source": [
        "def TextToTokens(text: str): # Function for cleaning inputs\n",
        "    currentEmotionList = list() # Creates empty list for current emotions\n",
        "    emotionDictionary = te.get_emotion(text) # Establishes an emotion dictionary of input\n",
        "    for emotion in emotionDictionary: # For each emotion in said dictiionary\n",
        "        currentEmotionList.append(emotionDictionary[emotion]) # Add to the current input list\n",
        "\n",
        "    text = CleanText(text) # Calls existing cleaning method from base\n",
        "    words = text.split() # Splits text into words\n",
        "    tokensList = list() # Makes list for each token\n",
        "    for word in words: # For every word\n",
        "        result = wordIndex.get(word,\"\") # Gets the index, nothing if not existing\n",
        "        if result != \"\": # If result isnt nothing\n",
        "             tokensList.append(result) # Adds to the token list\n",
        "    tokensList += currentEmotionList # Adds the lists together\n",
        "    return pad_sequences([tokensList],maxlen=encoderMaxLength,padding='post') # Returns the token list as padded to match encoder length for use in encoder"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCRtldTMPZ10"
      },
      "source": [
        "def CleanText(text): # Function for cleaning text, removing unneccesary characters and making lower case\n",
        "        \n",
        "    if type(text) == dict: # Makes sure text is not a dictionary (Common error due to : use in text)\n",
        "        keys = list(text) # Gets the key (note there should only be one )\n",
        "        totalText = \"\" # Empty to hold the entire text\n",
        "        for key in keys: # For each key\n",
        "            value = text[key] # Finds the value associaaed \n",
        "            text = f\"{key} + {value}\" # Appends data back together without colon\n",
        "            totalText = totalText + text # adds to total string\n",
        "        text = totalText # makes text the total string\n",
        "    text = text.lower() # Converts text input to lowercase\n",
        "    text = re.sub(r\"i'm\", \"i am\", text) # Removes potentially conflicting punctuation, swapping with full text\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text) # Removes conflicting characters completely\n",
        "        \n",
        "    return text # Return cleaned text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSy5TuceP1Qx"
      },
      "source": [
        "def PickleOpen(fileName): # Function to open items with picckle\n",
        "    with open(f'{fileName}.pickle', 'rb') as f: # Opens directory\n",
        "        item = pickle.load(f) # loads item\n",
        "        return item # Returns item"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kDYjoyIP8zm",
        "outputId": "80e6d686-2021-4176-f650-63641e48f414"
      },
      "source": [
        "encoderModel = models.load_model(\"encoderModel.h5\") # Load chatbot data\n",
        "decoderModel = models.load_model(\"decoderModel.h5\")\n",
        "decoderMaxLength = PickleOpen(\"decoderMaxLength\")\n",
        "encoderMaxLength = PickleOpen(\"encoderMaxLength\")\n",
        "wordIndex = PickleOpen(\"wordIndex\")\n",
        "indexWord = PickleOpen(\"indexWord\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}